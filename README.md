# AWS Serverless ETL Data Pipeline

This project implements a serverless ETL (Extract, Transform, Load) data pipeline using **AWS Lambda**, **Amazon S3**, **AWS RDS MySQL**, and **Step Functions**. The pipeline automates data preprocessing, storing processed data in MySQL, and archiving the data in Parquet format on S3.

## Architecture Overview

![Pipeline Diagram](https://github.com/aadhil96/AWS_Serverless_ETL_Data_Pipeline/blob/f2916463dd5babc482ea29cd63c56fae7ec3d29b/aws_data.drawio.png)

### Components:

#### 1. Landing-Data (S3 Data Source):
- This is the initial source of raw data, stored in Amazon S3 buckets.
- The Lambda function fetches the data from these buckets for preprocessing.

#### 2. Lambda Function for Data Preprocessing:
- This function is triggered when new data is available in the **Landing S3** bucket.
- It preprocesses the data (transforms, filters, cleans) and stores the processed result in another S3 bucket named **Processed-CSV-Data**.

#### 3. Processed-CSV-Data (S3 Processed Data):
- This bucket stores the processed CSV data generated by the preprocessing Lambda function.
- The data is ready for further processing or insertion into a database.

#### 4. Lambda Function for Processed Data to MySQL:
- This function reads the processed data from the **Processed-CSV-Data** S3 bucket and inserts it into an **Amazon RDS MySQL** database.
- The function also archives the data in **Parquet format** for optimized storage and later retrieval.

#### 5. Archive Data in Parquet Format:
- After the data is inserted into the MySQL database, this step archives the processed CSV data in **Parquet format** and stores it in the S3 bucket for long-term storage.

#### 6. AWS RDS MySQL Database:
- This is the final storage for the processed data.
- The Lambda function writes the cleaned and formatted data into **MySQL**, enabling queries and further analysis.

#### 7. Step Functions:
- The entire workflow is orchestrated using **AWS Step Functions**, ensuring the steps are executed sequentially and correctly.
- This includes triggering the Lambda functions and coordinating the data flow.




#### Running the Pipeline
- 1. Upload raw data to the Landing-Data S3 bucket.
- 2. The Lambda functions will automatically trigger, preprocess the data, insert it into MySQL, and archive the processed data as Parquet in S3.
- 3. AWS Step Functions will manage and coordinate the steps.

#### AWS Services Used
- AWS S3: For storing raw and processed data.
- AWS Lambda: For data preprocessing and database insertion.
- AWS RDS (MySQL): For relational database storage of processed data.
- AWS Step Functions: To manage and orchestrate the entire workflow.
  
#### Future Improvements
- Add more sophisticated error handling in Step Functions to retry steps in case of failure.
- Optimize Lambda functions for large datasets (e.g., through batch processing).
- Implement security best practices such as encryption and IAM roles with least privilege.
